{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-incidence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased',\n",
    "                                                    truncation=True,padding='max_length',max_length=512)\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train,X_test ,y_train,y_test = train_test_split(df['Content'].to_list(),\n",
    "                                                  df['label'].to_list(), random_state = 2020, \n",
    "                                                  test_size = 0.3,\n",
    "                                                  stratify=df['label'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, truncation=True, padding='max_length',max_length=512, return_tensors='tf')\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding='max_length',max_length=512, return_tensors='tf',)\n",
    "ytrain_encoded = tf.keras.utils.to_categorical(y_train, num_classes=2,dtype = 'float32')\n",
    "ytest_encoded = tf.keras.utils.to_categorical(y_test, num_classes=2,dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings=train_encodings['input_ids']\n",
    "test_encodings=test_encodings['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, loss='categorical_crossentropy', max_len=512):\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    #adding dropout layer\n",
    "    x = tf.keras.layers.Dropout(0.3)(cls_token)\n",
    "    #using a dense layer of 40 neurons as the number of unique categories is 40. \n",
    "    out = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n",
    "    #using categorical crossentropy as the loss as it is a multi-class classification problem\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=3e-5), loss=loss, metrics=F1_Score())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_layer = transformers.TFAutoModel.from_pretrained('distilbert-base-uncased')\n",
    "model = build_model(transformer_layer, max_len=512)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "Xtrain_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "  train_encodings,ytrain_encoded)).shuffle(64).repeat().batch(BATCH_SIZE).prefetch(AUTO)\n",
    " \n",
    "Xtest_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    test_encodings, ytest_encoded)).batch(4).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = len(X_train)// BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    Xtrain_dataset,\n",
    "    steps_per_epoch=n_steps,validation_data=Xtest_dataset,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions\n",
    "preds = model.predict(Xtest_dataset,verbose = 1)\n",
    "#converting the one hot vector output to a linear numpy array.\n",
    "pred_classes = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-squad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-duplicate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-diagram",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-zambia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
